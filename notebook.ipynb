{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackson/nLine/nline-data-api/.venv/lib/python3.12/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import polars as pl\n",
    "from data_api import get_raw, get_raw_dask, parse_datetime\n",
    "import pyarrow.dataset as ds\n",
    "from pyarrow.fs import GcsFileSystem\n",
    "\n",
    "gcs = GcsFileSystem()\n",
    "gcs_nline = \"gs://nline-public-data\"\n",
    "\n",
    "start, end =\"2023-10-01 00:00\", \"2023-10-01 02:00\"\n",
    "start = parse_datetime(start)\n",
    "end = parse_datetime(end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "source = \"nline-public-data/ghana/gridwatch_data/2023_period/\"\n",
    "\n",
    "time_filter = [(\"time\", \">=\", start), (\"time\", \"<\", end)]\n",
    "\n",
    "dataset = pq.ParquetDataset(source, filesystem=gcs, filters=time_filter)\n",
    "print(vars(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "from pyarrow.fs import GcsFileSystem\n",
    "import pandas as pd\n",
    "\n",
    "gcs = GcsFileSystem()\n",
    "source = \"nline-public-data/ghana/gridwatch_data/2023_period/\"\n",
    "\n",
    "# Define your time range\n",
    "start_time = pd.Timestamp(\"2023-01-01\")\n",
    "end_time = pd.Timestamp(\"2023-01-02\")\n",
    "\n",
    "# Create a dataset with a filter\n",
    "dataset = ds.dataset(source, filesystem=gcs, format=\"parquet\")\n",
    "filtered_dataset = dataset.filter((ds.field(\"time\") >= start_time) & (ds.field(\"time\") < end_time))\n",
    "\n",
    "# Read only the filtered data\n",
    "table = filtered_dataset.to_table()\n",
    "\n",
    "# Convert to Pandas or Polars if needed\n",
    "# df = table.to_pandas()\n",
    "# or\n",
    "df = pl.from_arrow(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# source = \"nline-public-data/ghana/gridwatch_data/2023_period/*.parquet\"\n",
    "source = \"gs://nline-public-data/ghana/gridwatch_data/2023_period/*.parquet\"\n",
    "\n",
    "dataset = ds.dataset(source, format=\"parquet\")\n",
    "fragments = list(dataset.get_fragments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gcs = GcsFileSystem()\n",
    "source = \"gs://nline-public-data/ghana/gridwatch_data/2023_period/*.parquet\"\n",
    "\n",
    "dataset = ds.dataset(source, filesystem=gcs, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(\"nline-public-data\")\n",
    "blobs = bucket.list_blobs(prefix=\"ghana/gridwatch_data/2023_period/\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_raw_dask(bucket_name, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_raw(\"ghana/gridwatch_data/full_period/\", \"2019-10-01 00:00\", \"2019-10-01 01:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow.dataset import dataset\n",
    "import gcsfs\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.fs import GcsFileSystem\n",
    "\n",
    "cloudfs = gcsfs.GCSFileSystem()\n",
    "source = \"gs://nline-public-data/ghana/gridwatch_data/full_period/*.parquet\"\n",
    "# reference multiple parquet files\n",
    "\n",
    "start, end =\"2019-10-01 00:00\", \"2019-10-01 01:00\"\n",
    "start = parse_datetime(start)\n",
    "end = parse_datetime(end)\n",
    "\n",
    "# load efficiently into polars\n",
    "ldf = pl.scan_parquet(source)\n",
    "ldf = ldf.filter((pl.col(\"time\") >= start) & (pl.col(\"time\") < end))\n",
    "ldf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_raw(bucket_name, start_time, end_time):\n",
    "    source = f\"{gcs_nline}/{bucket_name}\"\n",
    "    \n",
    "    # Convert start_time and end_time to datetime objects\n",
    "    start = parse_datetime(start_time)\n",
    "    end = parse_datetime(end_time)\n",
    "\n",
    "    dataset = ds.dataset(source, partitioning=\"hive\")\n",
    "    \n",
    "    df = pl.scan_pyarrow_dataset(\n",
    "        dataset\n",
    "    )\n",
    "\n",
    "    df_filtered = df.filter((pl.col(\"time\") >= start) & (pl.col(\"time\") < end))\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to write filtered data to disk\n",
    "def write_filtered_data(df, output_path):\n",
    "    df.sink_parquet(output_path)\n",
    "\n",
    "# Usage example\n",
    "bucket_name = \"ghana/gridwatch_data/full_period/\"\n",
    "start_time = \"2019-10-01 00:00\"\n",
    "end_time = \"2019-10-01 01:00\"\n",
    "output_path = \"filtered_data.parquet\"\n",
    "\n",
    "df = get_raw(bucket_name, start_time, end_time)\n",
    "# write_filtered_data(df, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "\n",
    "def get_raw_dask(bucket_name, start_time, end_time):\n",
    "    source = \"gs://nline-public-data/ghana/gridwatch_data/full_period/*.parquet\"\n",
    "\n",
    "    # source = f\"{gcs_nline}/{bucket_name}/*.parquet\"\n",
    "    \n",
    "    # Convert start_time and end_time to datetime objects\n",
    "    start = parse_datetime(start_time)\n",
    "    end = parse_datetime(end_time)\n",
    "\n",
    "    # Read the Parquet dataset\n",
    "    df = dd.read_parquet(source, engine='pyarrow')\n",
    "\n",
    "    # Apply time filter\n",
    "    # df_filtered = df[(df.time >= start) & (df.time < end)]\n",
    "    print(df.columns)\n",
    "    df_filtered = df[(df['time'] >= start) & (df['time'] < end)]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def write_filtered_data_dask(df, output_path):\n",
    "    df.to_parquet(output_path)\n",
    "\n",
    "# Usage example\n",
    "bucket_name = \"ghana/gridwatch_data/full_period/\"\n",
    "start_time = \"2019-10-01 00:00\"\n",
    "end_time = \"2019-10-01 01:00\"\n",
    "output_path = \"filtered_data_dask.parquet\"\n",
    "\n",
    "df = get_raw_dask(bucket_name, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
